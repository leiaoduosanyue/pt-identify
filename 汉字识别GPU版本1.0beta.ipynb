{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c68863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "753e8218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置设备（CPU或GPU）\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76f21adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "batch_sizes =64\n",
    "height = 64\n",
    "width = 64\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((height, width)),  # 调整图像大小\n",
    "    transforms.Grayscale(num_output_channels=1),  # 转换为灰度图像\n",
    "    transforms.ToTensor(),  # 转换为张量\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5]) # 标准化\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ffb5999",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: (48, 97, 43, 90)\n",
      "Image: [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]]), tensor([[[1.0000, 1.0000, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [0.9961, 0.9882, 0.9882,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [0.9961, 1.0000, 0.9922,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         ...,\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
      "         [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]), tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]]])]\n"
     ]
    }
   ],
   "source": [
    "# 自定义数据处理函数，将 PIL 图像转换为张量\n",
    "def custom_collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    images = [transforms.ToTensor()(image) for image in images]\n",
    "    return images, labels\n",
    "\n",
    "class TravelDatasets(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # 遍历数据集文件夹，获取图像文件和对应的标签文件\n",
    "        for filename in os.listdir(root_dir):\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                image_path = os.path.join(root_dir, filename)\n",
    "                label_path = os.path.join(root_dir, filename.replace(\".jpg\", \".txt\"))\n",
    "                self.image_paths.append(image_path)\n",
    "                self.labels.append(label_path)\n",
    "\n",
    "        self.label_to_int = {}  # 为训练数据集创建标签到整数的映射\n",
    "        for i, label in enumerate(self.labels):\n",
    "            with open(label, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                if text not in self.label_to_int:\n",
    "                    self.label_to_int[text] = i\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label_path = self.labels[idx]\n",
    "\n",
    "        # 加载图像\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # 加载标签，使用 'utf-8' 编码\n",
    "        with open(label_path, 'r', encoding='utf-8') as file:\n",
    "            label = file.read()\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, self.label_to_int[label]\n",
    "\n",
    "# 创建自定义数据集\n",
    "root_dir = r'D:\\Ai_Project_Tool\\Jupyter_notebook\\汉字识别GPU版本\\汉字数据集\\精简版1.0'\n",
    "custom_dataset = TravelDatasets(root_dir=root_dir, transform=None)\n",
    "\n",
    "# 创建数据加载器，指定自定义的数据处理函数\n",
    "data_loader = DataLoader(custom_dataset, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n",
    "\n",
    "# 创建测试数据集\n",
    "test_root_dir = r'D:\\Ai_Project_Tool\\Jupyter_notebook\\汉字识别GPU版本\\汉字数据集\\精简版1.0test'\n",
    "default_label_to_int = {}  # 在这里设置默认映射，未知标签将映射为-1\n",
    "test_dataset = TravelDatasets(root_dir=test_root_dir, transform=preprocess)\n",
    "\n",
    "# 创建测试数据加载器\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# 获取样本\n",
    "for images, labels in data_loader:\n",
    "    # 图像和标签在images和labels中\n",
    "    print(\"Label:\", labels)\n",
    "    # 图像显示出来\n",
    "    print(\"Image:\", images)\n",
    "    break  # 退出循循环，获取样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4673da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建数据集\n",
    "custom_dataset = TravelDatasets(root_dir, transform=preprocess)\n",
    "\n",
    "# 创建数据加载器\n",
    "data_loader = DataLoader(custom_dataset, batch_size=batch_sizes, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3df659c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# 构建深度学习模型\n",
    "class HandwritingRecognizer_chinese(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        # 调用父类构造函数\n",
    "        super(HandwritingRecognizer_chinese, self).__init__()\n",
    "        \n",
    "        # 卷积层1：1个输入通道，32个输出通道，3x3卷积核，填充1\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        \n",
    "        # 最大池化层：2x2的池化核，步幅2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # 全连接层1：32*32*32个输入特征，128个输出特征\n",
    "        self.fc1 = nn.Linear(32 * 32 * 32, 128)\n",
    "        \n",
    "        # 全连接层2：128个输入特征，num_classes个输出特征（根据分类数目）\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # 修改激活函数为 Leaky ReLU\n",
    "        # 0.1 是斜率，你可以根据需求调整\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 应用卷积1和Leaky ReLU，然后进行最大池化\n",
    "        x = self.pool(self.leaky_relu(self.conv1(x)))\n",
    "        \n",
    "        # 将特征张量展平\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # 应用全连接层1和Leaky ReLU\n",
    "        x = self.leaky_relu(self.fc1(x))\n",
    "        \n",
    "        # 应用全连接层2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6eadbc76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HandwritingRecognizer_chinese(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=32768, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=100, bias=True)\n",
       "  (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建模型实例\n",
    "num_classes = 100# 你的数据集中有n个不同的手写汉字\n",
    "model = HandwritingRecognizer_chinese(num_classes=num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b278e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6016ed4c",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250] Average Loss: 0.0039\n",
      "Epoch [2/250] Average Loss: 0.0038\n",
      "Epoch [3/250] Average Loss: 0.0038\n",
      "Epoch [4/250] Average Loss: 0.0038\n",
      "Epoch [5/250] Average Loss: 0.0038\n",
      "Epoch [6/250] Average Loss: 0.0037\n",
      "Epoch [7/250] Average Loss: 0.0037\n",
      "Epoch [8/250] Average Loss: 0.0036\n",
      "Epoch [9/250] Average Loss: 0.0037\n",
      "Epoch [10/250] Average Loss: 0.0035\n",
      "Epoch [11/250] Average Loss: 0.0036\n",
      "Epoch [12/250] Average Loss: 0.0035\n",
      "Epoch [13/250] Average Loss: 0.0034\n",
      "Epoch [14/250] Average Loss: 0.0035\n",
      "Epoch [15/250] Average Loss: 0.0034\n",
      "Epoch [16/250] Average Loss: 0.0033\n",
      "Epoch [17/250] Average Loss: 0.0033\n",
      "Epoch [18/250] Average Loss: 0.0033\n",
      "Epoch [19/250] Average Loss: 0.0032\n",
      "Epoch [20/250] Average Loss: 0.0032\n",
      "Epoch [21/250] Average Loss: 0.0032\n",
      "Epoch [22/250] Average Loss: 0.0032\n",
      "Epoch [23/250] Average Loss: 0.0031\n",
      "Epoch [24/250] Average Loss: 0.0031\n",
      "Epoch [25/250] Average Loss: 0.0030\n",
      "Epoch [26/250] Average Loss: 0.0031\n",
      "Epoch [27/250] Average Loss: 0.0030\n",
      "Epoch [28/250] Average Loss: 0.0030\n",
      "Epoch [29/250] Average Loss: 0.0030\n",
      "Epoch [30/250] Average Loss: 0.0029\n",
      "Epoch [31/250] Average Loss: 0.0029\n",
      "Epoch [32/250] Average Loss: 0.0029\n",
      "Epoch [33/250] Average Loss: 0.0028\n",
      "Epoch [34/250] Average Loss: 0.0028\n",
      "Epoch [35/250] Average Loss: 0.0028\n",
      "Epoch [36/250] Average Loss: 0.0028\n",
      "Epoch [37/250] Average Loss: 0.0027\n",
      "Epoch [38/250] Average Loss: 0.0028\n",
      "Epoch [39/250] Average Loss: 0.0027\n",
      "Epoch [40/250] Average Loss: 0.0027\n",
      "Epoch [41/250] Average Loss: 0.0027\n",
      "Epoch [42/250] Average Loss: 0.0026\n",
      "Epoch [43/250] Average Loss: 0.0026\n",
      "Epoch [44/250] Average Loss: 0.0027\n",
      "Epoch [45/250] Average Loss: 0.0026\n",
      "Epoch [46/250] Average Loss: 0.0026\n",
      "Epoch [47/250] Average Loss: 0.0025\n",
      "Epoch [48/250] Average Loss: 0.0025\n",
      "Epoch [49/250] Average Loss: 0.0025\n",
      "Epoch [50/250] Average Loss: 0.0025\n",
      "Epoch [51/250] Average Loss: 0.0025\n",
      "Epoch [52/250] Average Loss: 0.0024\n",
      "Epoch [53/250] Average Loss: 0.0024\n",
      "Epoch [54/250] Average Loss: 0.0025\n",
      "Epoch [55/250] Average Loss: 0.0024\n",
      "Epoch [56/250] Average Loss: 0.0023\n",
      "Epoch [57/250] Average Loss: 0.0023\n",
      "Epoch [58/250] Average Loss: 0.0023\n",
      "Epoch [59/250] Average Loss: 0.0023\n",
      "Epoch [60/250] Average Loss: 0.0024\n",
      "Epoch [61/250] Average Loss: 0.0023\n",
      "Epoch [62/250] Average Loss: 0.0023\n",
      "Epoch [63/250] Average Loss: 0.0022\n",
      "Epoch [64/250] Average Loss: 0.0023\n",
      "Epoch [65/250] Average Loss: 0.0022\n",
      "Epoch [66/250] Average Loss: 0.0022\n",
      "Epoch [67/250] Average Loss: 0.0022\n",
      "Epoch [68/250] Average Loss: 0.0022\n",
      "Epoch [69/250] Average Loss: 0.0022\n",
      "Epoch [70/250] Average Loss: 0.0022\n",
      "Epoch [71/250] Average Loss: 0.0021\n",
      "Epoch [72/250] Average Loss: 0.0021\n",
      "Epoch [73/250] Average Loss: 0.0021\n",
      "Epoch [74/250] Average Loss: 0.0021\n",
      "Epoch [75/250] Average Loss: 0.0021\n",
      "Epoch [76/250] Average Loss: 0.0020\n",
      "Epoch [77/250] Average Loss: 0.0020\n",
      "Epoch [78/250] Average Loss: 0.0020\n",
      "Epoch [79/250] Average Loss: 0.0020\n",
      "Epoch [80/250] Average Loss: 0.0020\n",
      "Epoch [81/250] Average Loss: 0.0020\n",
      "Epoch [82/250] Average Loss: 0.0019\n",
      "Epoch [83/250] Average Loss: 0.0019\n",
      "Epoch [84/250] Average Loss: 0.0019\n",
      "Epoch [85/250] Average Loss: 0.0019\n",
      "Epoch [86/250] Average Loss: 0.0019\n",
      "Epoch [87/250] Average Loss: 0.0019\n",
      "Epoch [88/250] Average Loss: 0.0019\n",
      "Epoch [89/250] Average Loss: 0.0018\n",
      "Epoch [90/250] Average Loss: 0.0019\n",
      "Epoch [91/250] Average Loss: 0.0019\n",
      "Epoch [92/250] Average Loss: 0.0018\n",
      "Epoch [93/250] Average Loss: 0.0018\n",
      "Epoch [94/250] Average Loss: 0.0018\n",
      "Epoch [95/250] Average Loss: 0.0018\n",
      "Epoch [96/250] Average Loss: 0.0017\n",
      "Epoch [97/250] Average Loss: 0.0017\n",
      "Epoch [98/250] Average Loss: 0.0017\n",
      "Epoch [99/250] Average Loss: 0.0017\n",
      "Epoch [100/250] Average Loss: 0.0017\n",
      "Epoch [101/250] Average Loss: 0.0017\n",
      "Epoch [102/250] Average Loss: 0.0017\n",
      "Epoch [103/250] Average Loss: 0.0017\n",
      "Epoch [104/250] Average Loss: 0.0017\n",
      "Epoch [105/250] Average Loss: 0.0017\n",
      "Epoch [106/250] Average Loss: 0.0016\n",
      "Epoch [107/250] Average Loss: 0.0016\n",
      "Epoch [108/250] Average Loss: 0.0016\n",
      "Epoch [109/250] Average Loss: 0.0016\n",
      "Epoch [110/250] Average Loss: 0.0016\n",
      "Epoch [111/250] Average Loss: 0.0016\n",
      "Epoch [112/250] Average Loss: 0.0016\n",
      "Epoch [113/250] Average Loss: 0.0016\n",
      "Epoch [114/250] Average Loss: 0.0016\n",
      "Epoch [115/250] Average Loss: 0.0016\n",
      "Epoch [116/250] Average Loss: 0.0015\n",
      "Epoch [117/250] Average Loss: 0.0015\n",
      "Epoch [118/250] Average Loss: 0.0015\n",
      "Epoch [119/250] Average Loss: 0.0015\n",
      "Epoch [120/250] Average Loss: 0.0015\n",
      "Epoch [121/250] Average Loss: 0.0015\n",
      "Epoch [122/250] Average Loss: 0.0015\n",
      "Epoch [123/250] Average Loss: 0.0015\n",
      "Epoch [124/250] Average Loss: 0.0015\n",
      "Epoch [125/250] Average Loss: 0.0015\n",
      "Epoch [126/250] Average Loss: 0.0015\n",
      "Epoch [127/250] Average Loss: 0.0014\n",
      "Epoch [128/250] Average Loss: 0.0014\n",
      "Epoch [129/250] Average Loss: 0.0014\n",
      "Epoch [130/250] Average Loss: 0.0014\n",
      "Epoch [131/250] Average Loss: 0.0014\n",
      "Epoch [132/250] Average Loss: 0.0014\n",
      "Epoch [133/250] Average Loss: 0.0014\n",
      "Epoch [134/250] Average Loss: 0.0014\n",
      "Epoch [135/250] Average Loss: 0.0014\n",
      "Epoch [136/250] Average Loss: 0.0014\n",
      "Epoch [137/250] Average Loss: 0.0014\n",
      "Epoch [138/250] Average Loss: 0.0014\n",
      "Epoch [139/250] Average Loss: 0.0013\n",
      "Epoch [140/250] Average Loss: 0.0013\n",
      "Epoch [141/250] Average Loss: 0.0013\n",
      "Epoch [142/250] Average Loss: 0.0013\n",
      "Epoch [143/250] Average Loss: 0.0013\n",
      "Epoch [144/250] Average Loss: 0.0013\n",
      "Epoch [145/250] Average Loss: 0.0013\n",
      "Epoch [146/250] Average Loss: 0.0013\n",
      "Epoch [147/250] Average Loss: 0.0013\n",
      "Epoch [148/250] Average Loss: 0.0013\n",
      "Epoch [149/250] Average Loss: 0.0013\n",
      "Epoch [150/250] Average Loss: 0.0013\n",
      "Epoch [151/250] Average Loss: 0.0013\n",
      "Epoch [152/250] Average Loss: 0.0013\n",
      "Epoch [153/250] Average Loss: 0.0012\n",
      "Epoch [154/250] Average Loss: 0.0012\n",
      "Epoch [155/250] Average Loss: 0.0012\n",
      "Epoch [156/250] Average Loss: 0.0012\n",
      "Epoch [157/250] Average Loss: 0.0012\n",
      "Epoch [158/250] Average Loss: 0.0012\n",
      "Epoch [159/250] Average Loss: 0.0012\n",
      "Epoch [160/250] Average Loss: 0.0012\n",
      "Epoch [161/250] Average Loss: 0.0012\n",
      "Epoch [162/250] Average Loss: 0.0012\n",
      "Epoch [163/250] Average Loss: 0.0012\n",
      "Epoch [164/250] Average Loss: 0.0011\n",
      "Epoch [165/250] Average Loss: 0.0011\n",
      "Epoch [166/250] Average Loss: 0.0011\n",
      "Epoch [167/250] Average Loss: 0.0011\n",
      "Epoch [168/250] Average Loss: 0.0011\n",
      "Epoch [169/250] Average Loss: 0.0011\n",
      "Epoch [170/250] Average Loss: 0.0011\n",
      "Epoch [171/250] Average Loss: 0.0011\n",
      "Epoch [172/250] Average Loss: 0.0011\n",
      "Epoch [173/250] Average Loss: 0.0011\n",
      "Epoch [174/250] Average Loss: 0.0011\n",
      "Epoch [175/250] Average Loss: 0.0011\n",
      "Epoch [176/250] Average Loss: 0.0011\n",
      "Epoch [177/250] Average Loss: 0.0011\n",
      "Epoch [178/250] Average Loss: 0.0011\n",
      "Epoch [179/250] Average Loss: 0.0011\n",
      "Epoch [180/250] Average Loss: 0.0011\n",
      "Epoch [181/250] Average Loss: 0.0011\n",
      "Epoch [182/250] Average Loss: 0.0011\n",
      "Epoch [183/250] Average Loss: 0.0010\n",
      "Epoch [184/250] Average Loss: 0.0011\n",
      "Epoch [185/250] Average Loss: 0.0011\n",
      "Epoch [186/250] Average Loss: 0.0010\n",
      "Epoch [187/250] Average Loss: 0.0010\n",
      "Epoch [188/250] Average Loss: 0.0010\n",
      "Epoch [189/250] Average Loss: 0.0010\n",
      "Epoch [190/250] Average Loss: 0.0010\n",
      "Epoch [191/250] Average Loss: 0.0010\n",
      "Epoch [192/250] Average Loss: 0.0010\n",
      "Epoch [193/250] Average Loss: 0.0010\n",
      "Epoch [194/250] Average Loss: 0.0010\n",
      "Epoch [195/250] Average Loss: 0.0010\n",
      "Epoch [196/250] Average Loss: 0.0010\n",
      "Epoch [197/250] Average Loss: 0.0010\n",
      "Epoch [198/250] Average Loss: 0.0010\n",
      "Epoch [199/250] Average Loss: 0.0010\n",
      "Epoch [200/250] Average Loss: 0.0010\n",
      "Epoch [201/250] Average Loss: 0.0010\n",
      "Epoch [202/250] Average Loss: 0.0009\n",
      "Epoch [203/250] Average Loss: 0.0010\n",
      "Epoch [204/250] Average Loss: 0.0010\n",
      "Epoch [205/250] Average Loss: 0.0009\n",
      "Epoch [206/250] Average Loss: 0.0009\n",
      "Epoch [207/250] Average Loss: 0.0009\n",
      "Epoch [208/250] Average Loss: 0.0009\n",
      "Epoch [209/250] Average Loss: 0.0009\n",
      "Epoch [210/250] Average Loss: 0.0009\n",
      "Epoch [211/250] Average Loss: 0.0009\n",
      "Epoch [212/250] Average Loss: 0.0009\n",
      "Epoch [213/250] Average Loss: 0.0009\n",
      "Epoch [214/250] Average Loss: 0.0009\n",
      "Epoch [215/250] Average Loss: 0.0009\n",
      "Epoch [216/250] Average Loss: 0.0009\n",
      "Epoch [217/250] Average Loss: 0.0009\n",
      "Epoch [218/250] Average Loss: 0.0009\n",
      "Epoch [219/250] Average Loss: 0.0009\n",
      "Epoch [220/250] Average Loss: 0.0009\n",
      "Epoch [221/250] Average Loss: 0.0009\n",
      "Epoch [222/250] Average Loss: 0.0009\n",
      "Epoch [223/250] Average Loss: 0.0009\n",
      "Epoch [224/250] Average Loss: 0.0008\n",
      "Epoch [225/250] Average Loss: 0.0008\n",
      "Epoch [226/250] Average Loss: 0.0008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [227/250] Average Loss: 0.0009\n",
      "Epoch [228/250] Average Loss: 0.0008\n",
      "Epoch [229/250] Average Loss: 0.0008\n",
      "Epoch [230/250] Average Loss: 0.0008\n",
      "Epoch [231/250] Average Loss: 0.0008\n",
      "Epoch [232/250] Average Loss: 0.0008\n",
      "Epoch [233/250] Average Loss: 0.0008\n",
      "Epoch [234/250] Average Loss: 0.0008\n",
      "Epoch [235/250] Average Loss: 0.0008\n",
      "Epoch [236/250] Average Loss: 0.0008\n",
      "Epoch [237/250] Average Loss: 0.0008\n",
      "Epoch [238/250] Average Loss: 0.0008\n",
      "Epoch [239/250] Average Loss: 0.0008\n",
      "Epoch [240/250] Average Loss: 0.0008\n",
      "Epoch [241/250] Average Loss: 0.0008\n",
      "Epoch [242/250] Average Loss: 0.0008\n",
      "Epoch [243/250] Average Loss: 0.0008\n",
      "Epoch [244/250] Average Loss: 0.0008\n",
      "Epoch [245/250] Average Loss: 0.0008\n",
      "Epoch [246/250] Average Loss: 0.0008\n",
      "Epoch [247/250] Average Loss: 0.0008\n",
      "Epoch [248/250] Average Loss: 0.0008\n",
      "Epoch [249/250] Average Loss: 0.0008\n",
      "Epoch [250/250] Average Loss: 0.0008\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "num_epochs = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for images, labels in data_loader:\n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        labels = torch.tensor([int(label) for label in labels]).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    average_loss = epoch_loss / len(data_loader)\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Average Loss: {average_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf3d305a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "torch.save(model.state_dict(), \"handwriting_recognizer_chinese_beta1.0.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e64a12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
